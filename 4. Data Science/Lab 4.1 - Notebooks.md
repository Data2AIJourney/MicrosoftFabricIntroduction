## ğŸ¯ Objective
Explore the use of Notebooks in Fabric to run Spark-based transformations.

## âœ… Prerequisites
- Existing Lakehouse with data loaded

## ğŸ“ Step 1: Create a Notebook
1. Click New â†’ Notebook
2. Name it Lakehouse_Analysis
3. Select Spark runtime

## ğŸ§ª Step 2: Load Data from Lakehouse
1. df = spark.read.table("Sales_Lakehouse.sales_data")
2. df.show()

## ğŸ§¹ Step 3: Transform the Data
1. df_filtered = df.filter(df["region"] == "Midwest")
2. df_filtered.groupBy("product").sum("sales").show()

## ğŸ“˜ Key Concepts
- Notebook: Interactive environment for Spark, Python, and SQL
- DataFrame: Core abstraction for data in Spark
